
---
title: 'Principled Bayesian Workflow: Practicing Safe Bayes.'
author: "Keith O'Rourke, Health Canada"
date: "August 2, 2019"
output: beamer_presentation
---

So now you can Bayes - need to do it safely (especially in a regulatory environment).
========================================================

- Motivated by courses I gave in Health Canada in 2010/11 - "resulted" in less than critical review of submissions.  
- Today is largely to provide motivation to read and work through material/programs developed by Michael Betancourt on Principled Bayesian Workflow.
- [GitHub for Rstan LINK](https://github.com/betanalpha/knitr_case_studies/tree/master/principled_bayesian_workflow)
- [GitHub for Pystan LINK](https://github.com/betanalpha/jupyter_case_studies/tree/master/principled_bayesian_workflow)
- I'll do a largely conceptual introduction and work through a "toyed down" example. [Files on Github LINK.](https://github.com/KeithORourke/BayesinWorkflowLecture)

My relevant background. 
========================================================

- Provided statistical support and mentor-ship to research fellows at the University of Toronto and Toronto Hospital (1985-1998).
- Had no degree in statistics but still taught one course and gave multiple tutorials. 
- Was recruited to the Ottawa Hospital (1998-2001).
- Did a DPhil in Statistics at Oxford (2001-2007). 
- Visited the Statistical Sciences Department at Duke University, *the world's leading center for Bayesian statistics* (2007-2008).

My relevant background (cont.)
========================================================

- Joined Health Canada 2009, gave Bayesian Courses in 2010/11.
- Currently at Pest Management Regulatory Agency.
- Gave webinars on Bayes for the American Statistical Society and CSEB in 2012 (first reviewed positively, not the second).
- Currently an author on [Statistical Modeling, Causal Inference, and Social Science LINK](https://statmodeling.stat.columbia.edu/author/keithor/) a mostly Bayesian perspective blog. 

Some of my early experience in Bayesian work.
========================================================

- Used someone's published Bayesian model - obtained positive posterior probabilities for negative proportions.
- The priors they had specified were independent even though proportions were dependent.
- Contacted the author - they were unconcerned.
- Claimed it was a result of my not having adequate data and so no need to worry. 
- Sometimes it is hard to know when you have adequate data but the prior is obviously wrong and *should* be fixed.
- (This sort of thing was be done a lot in early 2000s given limitations).


Conceptual review of Bayes. 
========================================================
Seeing all of Bayes all at once.

- Need to go back and focus on basics - not always popular. 
- Once told that they hide the best techniques in martial arts in the beginners form. 
- Two stage simulation (described later) is not a cheap trick but rather a way to *really, really* understand (I first came up with it in 2004). Told I must be wrong.
- Later learned Don Rubin (Harvard) came up with it 1984. 

Conceptual review of Bayes (cont.) 
========================================================
Seeing all of Bayes all at once.

- Andrew Gelman blogged numerous times - in Bayes you write down the model and simply watch it work (crickets...).
- From a two stage sampling perspective it is obvious. 
- But so much more!
- Or as Michael Betancourt put it: Work it, Make it, Do it, Makes Us Harder, Better, Faster, Stronger.

Conceptual review of Bayes (cont.) 
========================================================

- Numerous (wild?) claims about Bayes.  
- For instance in an American Statistical Association webinar (2010/11).
- "No need to worry if you are unsure about the prior - lots of noninformtive priors that work just great."
- "... the term “noninformative” appears to be a Whorfian trick of language. Just as an “empty” gasoline drum is more dangerous than a “full” one (Whorf 1941), a noninformative prior may distort estimates more than would an informative prior." (McElreath and Koster, 2014)
- **Take no one's word for it!**


Need to worry if you are unsure about the prior.
========================================================

- One of the first I heard about was from Peter F. Thall at M.D. Anderson Cancer Center. 
- Not obvious at the time perhaps given the lack of awareness of the dangers of (silly?) default flat (noninformative) priors. 
- In 2011 I emailed him and got back *It seems that we are both in the How to tell if you have a prior with undesirable properties business. I got into this years ago when I was doing CRM dose finding with the model logit{Prob(toxicity | dose )} = a + b log(dose) and just assumed _noninformative_ normal priors on  a and b, with variance = 100.  The resulting CRM design does strange things with the first 3 to 6 patients. It drove me and my programmer crazy for a month.*
- See [Stan Prior Choice Recommendations wiki LINK.](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations)


Conceptual review of statistics. 
========================================================
Seeing all of statistics all at once.

- In both Bayes and Frequentist statistics the holy grail is clearly seeing what repeatedly would happen when trying to learn from observations like the ones you have in hand.
- A helpful? metaphor of discerning what cast the shadows. 
- Think of learning about an object just from the shadows it casts while being unable to look directly at the object.
- We see those shadows but really are only interested in what is casting them. They may look very scary, but the object casting them maybe mice. (All too common to take noisy observations as the reality that generated them.)


Strategy: Generate realistic Fake Universes to better grasp reality.  
========================================================

- Make something where you can easily "see" what would repeatedly happen.
- In analytical chemistry you can repeatedly spike test tubes with know trace amounts of X.
- In statistics we can't repeatedly spike humans with say know faint rates. 
- So we have to represent faint rates abstractly - make a fake universe were you set the rate. 
- Most statisticians prefer to use probability models (and math) to do that.

Strategy: Generate realistic Fake Universes to better grasp reality (cont.) 
========================================================

- Today almost anyone can use probability models (and simulation!!!) to make fake universes.
- Math: The exact study of ideal states of things (a fake universe).
- Simulation: The approximate study of the same.
- Two stage simulation is the just the Bayesian version of generating realistic enough? fake universes!
- My toy example today will be annual faint rates in business leaders "rounds" at Toronto Hospital where 3 out of 9 fainted the one year I heard about it.

Lets plot the set of fake universes.
========================================================
Two stage simulation, simulate the faint rate and then number of faints given that rate. 

```{r simpleSim1 , echo=FALSE, cache = TRUE}
p <- table(round(rbeta(100000,.5,.5),1))[c(-1,-11)]
prior <- p/sum(p)

foo <- function(p,i=(1:9)/10){
  fakeP <- sample(i,1,prob = p)
  fakeX <- rbinom(1,9,prob = fakeP)
  return(c(fakeP,fakeX))
}

fakeUs <- replicate(10000,foo(p=prior))

fakeXsbyFakePs <- split(fakeUs[2,],fakeUs[1,])

z=table(fakeUs[2,],fakeUs[1,])

mat <- matrix(nrow=90,ncol=3,NA)
k <- 1 
for(i in 1:nrow(z)) {
  for(j in 1:ncol(z)){
    mat[k,] <- c(i,j,z[i,j])
    k <- k + 1 }
}


mat <- data.frame(mat)
dimnames(mat)[[2]] <- c("Faints","Universe","Count")
mat$Universe <- mat$Universe/10
mat$Faints <- factor(mat$Faints - 1)
mat$Count <- mat$Count/sum(mat$Count)

ourUniverse <- ifelse(mat$Faints == 3,2,1)

library("scatterplot3d")
scatterplot3d(mat[,c(2,1,3)], pch = 16, type="h",main="Set of Fake Universes for Fainting.")

```


Why make fake universes of faint rates and occurrences?
========================================================

- Once the joint model is set - the prior and data generating model - Bayes is deductive. 
- The joint model is the first premise, the data is the second premise and the posterior is the conclusion.
- Assuming adequate sampling from the posterior - the quality of what follows totally depends the premises.
- *Can be very bad if the premises are deficient (either joint model or data).*

Picture Bayes as deductive.
========================================================
First premise on the left - do they adequately represent our grasp of reality (as in a artist's pencil sketch)? Note the conclusion in red on the right is clearly contained in the premises (QED).


```{r simpleSimLastMinute , echo=FALSE, cache = TRUE}
p <- table(round(rbeta(100000,.5,.5),1))[c(-1,-11)]
prior <- p/sum(p)

foo <- function(p,i=(1:9)/10){
  fakeP <- sample(i,1,prob = p)
  fakeX <- rbinom(1,9,prob = fakeP)
  return(c(fakeP,fakeX))
}

fakeUs <- replicate(10000,foo(p=prior))

fakeXsbyFakePs <- split(fakeUs[2,],fakeUs[1,])

z=table(fakeUs[2,],fakeUs[1,])

mat <- matrix(nrow=90,ncol=3,NA)
k <- 1 
for(i in 1:nrow(z)) {
  for(j in 1:ncol(z)){
    mat[k,] <- c(i,j,z[i,j])
    k <- k + 1 }
}


mat <- data.frame(mat)
dimnames(mat)[[2]] <- c("Faints","Universe","Count")
mat$Universe <- mat$Universe/10
mat$Faints <- factor(mat$Faints - 1)
mat$Count <- mat$Count/sum(mat$Count)

ourUniverse <- ifelse(mat$Faints == 3,2,1)

library("scatterplot3d")
par(mfrow = c(1,2))
scatterplot3d(mat[,c(2,1,3)], pch = 16, type="h",main="Set of Fake Universes for Fainting.")
scatterplot3d(mat[,c(2,1,3)], pch = 16, type="h",color = ourUniverse,main="Note the posterior it implies, given 3 out  of 9")

```

Consistent with domain expertise?
========================================================
Note the 9 fake universes on the x axis and 10 possible faint totals on the y axis with how often on the z axis. A proportional set of fake universes and faint totals that we judge could be encountered in topics *like this* (Bayes/Frequency reference sets). 

```{r simpleSim2 , echo=FALSE, cache = TRUE, dependson = "simpleSim" }
library("scatterplot3d")
par(mfrow = c(1,1))
scatterplot3d(mat[,c(2,1,3)], pch = 16, type="h",color = ourUniverse,main="The proportional subset of universes most compatible with our universe shown in red")

```


Things to consider on the last slides.
========================================================

- Are these fake universes and repeatedly happening faint totals roughly like our universe?
- Biggest fail so far may be air density greater than concrete in air quality modeling.
- *If not too wrong, provides an ample labratory to evaluate the performance of the given joint model and data!*
- Often a good idea to use different joint model to create the fake universes to evaluate the Bayesian model actually being used in the analysis.


Marginal views and disagreements on views.
========================================================

```{r simpleSim3b, echo=FALSE, cache = TRUE, fig.height = 4}
# Plot
library(ggplot2)
tt <- table(fakeUs[1,])
data1 <- data.frame(tt)
dimnames(data1)[[2]] <- c("x","y")
data1$which <- "MarginalPrior"
levels(data1$x) <- 1:9
  
tt <- table(fakeUs[2,])
data2 <- data.frame(tt)
dimnames(data2)[[2]] <- c("x","y")
data2$which <- "PriorPredictive"

data <- rbind(data2,data1)

ggplot(data, aes(x=x, y=y)) +
  geom_point() + 
  geom_segment( aes(x=x, xend=x, y=0, yend=y)) +
  ggtitle("Prior Generated Distributions") + 
  facet_wrap(~ which, nrow = 1) + xlab("Percentage in number out of 10 on left, number out of 9 on right.")

```

- Prior predictive distribution is on the right - seem realistic?
- Marginal prior distribution over all universes is on the left - do these parameters seem realistic.
- Some argue only look at the second (which has advantages especially with many parameters) while I would look at both. 


But so much more to make of the set of fake universes generated.
========================================================

```{r simpleSim3a, echo=FALSE,cache = TRUE, dependson = "simpleSim1"}

sample3 <- t(fakeUs[,fakeUs[2,] == 3])
sampleNot3 <- t(fakeUs[,fakeUs[2,] != 3])
subSample <- data.frame(sample3[1:3,],sampleNot3[1:3,])
dimnames(subSample)[[2]] <- c("TrueProp1","3 Faints","TrueProp2","Not 3 Faints")
subSample[1:2,]

```

- Samples on the left are valid posterior samples - right?
- What are the ones on the right a valid posterior sample from?
- But you know how to get many posterior samples using MCMC.
- If your MCMC is valid the ranks of two stage sample with the MCMC sampleS (say 500) should be uniform. 
- Now we can check the MCMC sampling for every point in the fake universes generated (true parameter, valid sample).

Three tasks in principled Bayesian workflow to be safer.
========================================================

- Three tasks in Bayesian Workflow - assess adequacy of joint model, posterior sampling and data. 
- Assessing the adequacy of joint model has only become practical in the past 5 or so years. 
- Around 2012 even experts were refusing (even saying not kosher in Bayes). 
- Still some resistance to assessing the adequacy of the data?
- Lots can and did go wrong and it remains the most challenging aspect (and an incentive to just ignore it?).


What two stage fake Bayesian universe generation enables.  
========================================================
Makes safe Bayes practical (was not until recently!).

- Domain Expertise Consistency: Is our model consistent with our domain expertise?
- Computational Faithfulness: Are our computational tools sufficient to accurately fit the model?
- Model Sensitivity: How do we expect our inferences to perform over the possible samples in fake universes?
- (Model Adequacy: Is our model rich enough to capture the relevant structure of our universe?)


A more sensible set of fake universes?
========================================================

```{r simpleSim4, echo=FALSE, cache = TRUE}
p <- table(round(rbeta(100000,1,3),1))[c(-1,-11)]
prior <- p/sum(p)

foo <- function(p,i=(1:9)/10){
  fakeP <- sample(i,1,prob = p)
  fakeX <- rbinom(1,9,prob = fakeP)
  return(c(fakeP,fakeX))
}

fakeUs <- replicate(10000,foo(p=prior))

fakeXsbyFakePs <- split(fakeUs[2,],fakeUs[1,])

z=table(fakeUs[2,],fakeUs[1,])

mat <- matrix(nrow=90,ncol=3,NA)
k <- 1 
for(i in 1:nrow(z)) {
  for(j in 1:ncol(z)){
    mat[k,] <- c(i,j,z[i,j])
    k <- k + 1 }
}


mat <- data.frame(mat)
dimnames(mat)[[2]] <- c("Faints","Universe","Count")
mat$Universe <- mat$Universe/10
mat$Faints <- factor(mat$Faints - 1)
mat$Count <- mat$Count/sum(mat$Count)

ourUniverse <- ifelse(mat$Faints == 3,2,1)

library("scatterplot3d")
par(mfrow = c(1,2))
scatterplot3d(mat[,c(2,1,3)], pch = 16, type="h",main="Set of Fake Universes for Fainting.")
scatterplot3d(mat[,c(2,1,3)], pch = 16, type="h",color = ourUniverse,main="Note the posterior it implies, given 3 out  of 9")

```

Domain Expertise Consistency: Is our model consistent with our domain expertise?
========================================================
Now use Rstan which has prior predictive functions (used to have to leave out data ~ statement in model block).

```{r rstan1, echo=FALSE, results = "hide", message = FALSE, warning = FALSE ,cache = TRUE }
library(rstan)
rstan_options(auto_write = TRUE)

library(foreach)
library(doParallel)

util <- new.env()
source('stan_utility.R', local=util)

c_light <- c("#DCBCBC")
c_light_highlight <- c("#C79999")
c_mid <- c("#B97C7C")
c_mid_highlight <- c("#A25050")
c_dark <- c("#8F2727")
c_dark_highlight <- c("#7C0000")

modelString = "
data {
int T;
}

generated quantities {
// Simulate model configuration from prior model
real<lower=0,upper=1> theta = beta_rng(2,4);

// Simulate data from observational model
int y;
  y = binomial_rng(T, theta);
}
"
# Compile the posterior fit model
fit_model = stan_model(model_code=modelString)

R <- 10000


simu_data <- list("T" = 9)

fit <- sampling( object=fit_model , 
  data = simu_data , 
  chains = 1 ,
  iter = R , 
  warmup = 0 , algorithm="Fixed_param" )

simu_thetas <- extract(fit)$theta
simu_ys <- extract(fit)$y

joint <- data.frame(Universe=simu_thetas,Faints=simu_ys)


z=table(round(joint$Universe,2),joint$Faints)


mat <- matrix(nrow=990,ncol=3,NA)
k <- 1 
for(i in 1:nrow(z)) {
  for(j in 1:ncol(z)){
    mat[k,] <- as.numeric(c(dimnames(z)[[1]][i],dimnames(z)[[2]][j],z[i,j]))
    k <- k + 1 }
}


mat <- data.frame(mat)
dimnames(mat)[[2]] <- c("Universe","Faints","Count")
ourUniverse <- ifelse(mat$Faints == 3,2,1)

library("scatterplot3d")
par(mfrow = c(1,2))
scatterplot3d(mat, pch = 16, type="h")

scatterplot3d(mat, pch = 16, type="h",color = ourUniverse)

```


Marginal views: Marginal Prior
========================================================

```{r rstan2a, echo=FALSE, cache = TRUE, dependson = "stan1"}
# Plot


ggplot(joint, aes(Universe)) +
  geom_density(kernel = "cosine") + 
  ggtitle("Prior Distribution")

```

- Do these parameters seem realistic?


Marginal views: Prior predictive distribution.
========================================================

```{r rstan2b, echo=FALSE, cache = TRUE, dependson = "stan1"}
# Plot

ggplot(joint, aes(Faints)) +
  geom_bar() + 
  ggtitle("Prior Predictive Distribution")

```

- Prior predictive distribution - these observations seem realistic?


Computational Faithfulness: Are our computational tools sufficient to accurately fit the model?
========================================================
Lets look at some samples of the two stage simulation

```{r rstan3, echo=FALSE,cache = TRUE, dependson = "stan1"}

sample3 <- joint[joint[,2] == 3,] 
sampleNot3 <- joint[joint[,2] != 3,] 
(cbind(sample3[1:3,],sampleNot3[1:3,]))


```

- If the MCMC is valid the ranks of two stage sample with the MCMC sampleS should be uniform. 
- Now we can check the MCMC sampling for every point in the fake universes generated (true parameter, valid sample).

Computational Faithfulness - are ranks uniform?
========================================================
Lets obtain 500 MCMC samples for each two stage sample in Rstan. What went wrong? My priors did not match.

```{r rstan4a, echo=FALSE, results = "hide", message = FALSE, warning = FALSE ,cache = TRUE, dependson = "stan1"}

modelString = "
data {
  int y;
}

parameters {
  real<lower=0,upper=1> theta; // Probability of fainting
}

model {
  theta ~ beta(1,3); // Prior model
  y ~ binomial(9,theta);         // Observational model
}
"
fit_model = stan_model(model_code=modelString)


hold_thetas <- simu_thetas
hold_ys <- simu_ys
hold_R <- R

subsample <- 10000
simu_thetas <- simu_thetas[1:subsample]
simu_ys <- simu_ys[1:subsample]
R <- subsample

tryCatch({
  registerDoParallel(makeCluster(detectCores()))
  
  simu_list <- t(data.matrix(data.frame(simu_thetas, simu_ys)))
  
  # Compile the posterior fit model
  fit_model = stan_model(model_code=modelString)
  
  ensemble_output <- foreach(simu=simu_list,
    .combine='cbind') %dopar% {
      simu_theta <- simu[1]
      simu_y <- simu[2];
      
      # Fit the simulated observation
      input_data <- list("y" = simu_y)
      
      capture.output(library(rstan))
      capture.output(fit <- sampling(fit_model, data=input_data, seed=4938483))
      
      # Compute diagnostics
      util <- new.env()
      source('stan_utility.R', local=util)
      
      warning_code <- util$check_all_diagnostics(fit, quiet=TRUE)
      
      # Compute rank of prior draw with respect to thinned posterior draws
      sbc_rank <- sum(simu_theta < extract(fit)$theta[seq(1, 4000 - 8, 8)])
      
      # Compute posterior sensitivities
      s <- summary(fit, probs = c(), pars='theta')$summary
      post_mean_theta <- s[,1]
      post_sd_theta <- s[,3]
      
      # From assumed Beta prior
      a <- 1; b <- 3
      prior_sd_theta <- sqrt(a * b/((a+b)^2 * (a+b+1)))
      
      z_score <- (post_mean_theta - simu_theta) / post_sd_theta
      shrinkage <- 1 - (post_sd_theta / prior_sd_theta)**2
      
      c(warning_code, sbc_rank, z_score, shrinkage)
    }
}, finally={ stopImplicitCluster() })


sbc_rank <- ensemble_output[2,]
sbc_hist <- hist(sbc_rank, seq(0, 500, 25) - 0.5,
  col=c_dark, border=c_dark_highlight, plot=FALSE)
plot(sbc_hist, main="", xlab="Prior Rank", yaxt='n', ylab="")

low <- qbinom(0.005, R, 1 / 20)
mid <- qbinom(0.5, R, 1 / 20)
high <- qbinom(0.995, R, 1 / 20)
bar_x <- c(-10, 510, 500, 510, -10, 0, -10)
bar_y <- c(high, high, mid, low, low, mid, high)

polygon(bar_x, bar_y, col=c("#DDDDDD"), border=NA)
segments(x0=0, x1=500, y0=mid, y1=mid, col=c("#999999"), lwd=2)

plot(sbc_hist, col=c_dark, border=c_dark_highlight, add=T)

simu_thetas <- hold_thetas
simu_ys <- hold_ys
```

Now, lets do that correctly in Stan
========================================================
Lets obtain 500 MCMC samples for each two stage sample in Rstan using same priors.

```{r rstan4b, echo=FALSE, results = "hide", message = FALSE, warning = FALSE ,cache = TRUE, dependson = "stan1"}

modelString = "
data {
  int y;
}

parameters {
  real<lower=0,upper=1> theta; // Probability of fainting
}

model {
  theta ~ beta(2,4); // Prior model
  y ~ binomial(9,theta);         // Observational model
}
"
fit_model = stan_model(model_code=modelString)


hold_thetas <- simu_thetas
hold_ys <- simu_ys
hold_R <- R

subsample <- 10000
simu_thetas <- simu_thetas[1:subsample]
simu_ys <- simu_ys[1:subsample]
R <- subsample

tryCatch({
  registerDoParallel(makeCluster(detectCores()))
  
  simu_list <- t(data.matrix(data.frame(simu_thetas, simu_ys)))
  
  # Compile the posterior fit model
  fit_model = stan_model(model_code=modelString)
  
  ensemble_output <- foreach(simu=simu_list,
    .combine='cbind') %dopar% {
      simu_theta <- simu[1]
      simu_y <- simu[2];
      
      # Fit the simulated observation
      input_data <- list("y" = simu_y)
      
      capture.output(library(rstan))
      capture.output(fit <- sampling(fit_model, data=input_data, seed=4938483))
      
      # Compute diagnostics
      util <- new.env()
      source('stan_utility.R', local=util)
      
      warning_code <- util$check_all_diagnostics(fit, quiet=TRUE)
      
      # Compute rank of prior draw with respect to thinned posterior draws
      sbc_rank <- sum(simu_theta < extract(fit)$theta[seq(1, 4000 - 8, 8)])
      
      # Compute posterior sensitivities
      s <- summary(fit, probs = c(), pars='theta')$summary
      post_mean_theta <- s[,1]
      post_sd_theta <- s[,3]
      
      # From assumed Beta prior
      a <- 2; b <- 4
      prior_sd_theta <- sqrt(a * b/((a+b)^2 * (a+b+1)))
      
      z_score <- (post_mean_theta - simu_theta) / post_sd_theta
      shrinkage <- 1 - (post_sd_theta / prior_sd_theta)**2
      
      c(warning_code, sbc_rank, z_score, shrinkage)
    }
}, finally={ stopImplicitCluster() })



sbc_rank <- ensemble_output[2,]
sbc_hist <- hist(sbc_rank, seq(0, 500, 25) - 0.5,
  col=c_dark, border=c_dark_highlight, plot=FALSE)
plot(sbc_hist, main="", xlab="Prior Rank", yaxt='n', ylab="")

low <- qbinom(0.005, R, 1 / 20)
mid <- qbinom(0.5, R, 1 / 20)
high <- qbinom(0.995, R, 1 / 20)
bar_x <- c(-10, 510, 500, 510, -10, 0, -10)
bar_y <- c(high, high, mid, low, low, mid, high)

polygon(bar_x, bar_y, col=c("#DDDDDD"), border=NA)
segments(x0=0, x1=500, y0=mid, y1=mid, col=c("#999999"), lwd=2)

plot(sbc_hist, col=c_dark, border=c_dark_highlight, add=T)

simu_thetas <- hold_thetas
simu_ys <- hold_ys
```

Now actually you should first run a set of utility diagnostic checks. 
========================================================


```{r  rstan5a, cache = TRUE, dependson = "stan4b"}


warning_code <- ensemble_output[1,]
if (sum(warning_code) != 0) {
  print ("Some simulated posterior fits in the joint ensemble encountered problems!")
  for (r in 1:R) {
    if (warning_code[r] != 0) {
      print(sprintf('Replication %s of %s', r, R))
      util$parse_warning_code(warning_code[r])
      print(sprintf('Simulated theta = %s', simu_thetas[r]))
      print(" ")
    }
  }
} else {
  print ("No posterior fits in the joint ensemble encountered problems!")
}


```

Model Sensitivity. 
========================================================
Generic Posterior Shrinkage and z-Scores.

```{r rstan6a, echo=FALSE ,cache = TRUE, dependson = "stan4b"}


z_score <- ensemble_output[3,]
shrinkage <- ensemble_output[4,]

plot(shrinkage, z_score, col=c("#8F272720"), lwd=2, pch=16, cex=0.8,
  xlim=c(0, 1), xlab="Posterior Shrinkage (1 - post.var/prior.var)",
  ylim=c(-5, 5), ylab="Posterior z-Score ((post.mean - truth)/post.sd")


```

Model Sensitivity: More focused assessments.
========================================================

- [FDA 2010 guidance LINK](https://www.fda.gov/media/71512/download)
- "FDA usually recommends you provide simulations of your trial at the planning (or
IDE) stage. This will facilitate FDA’s assessment of the operating characteristics of
the Bayesian trial; specifically, the type I and type II error rates." 
- Actually carried out a feasibility study of calculating the type I and type II error rates with a major pharma consulting firm and Yongmin Yu at Health Canada in 2012. 


Pick one universe and repeatedly sample and see what happens.
========================================================

- Think of needing to know if faint rate not >= 50%.
- Set the probability of fainting to .5 and simulate data just for that fake universe.
- Type 1 error? See how often ((post_mean_theta - .5) / post_sd_theta) < -1.65.
- Calculate the 90th percentile of posterior probability.
- See how often that is claimed < 50%.


What repeatedly happens in this fake universe of 50% fainting.
========================================================


```{r rstan6b, echo=FALSE ,cache = TRUE, message = FALSE, warning = FALSE }

modelString = "
data {
  int y;
}

parameters {
  real<lower=0,upper=1> theta; // Probability of fainting
}

model {
  theta ~ beta(2,4); // Prior model
  y ~ binomial(9,theta);         // Observational model
}
"
fit_model = stan_model(model_code=modelString)


simu_thetas <- rep(.5,1000)
simu_ys <- rbinom(1000,9,prob = .5)
R <- 1000

tryCatch({
  registerDoParallel(makeCluster(detectCores()))
  
  simu_list <- t(data.matrix(data.frame(simu_thetas, simu_ys)))
  
  # Compile the posterior fit model
  fit_model = stan_model(model_code=modelString)
  
  ensemble_output <- foreach(simu=simu_list,
    .combine='cbind') %dopar% {
      simu_theta <- simu[1]
      simu_y <- simu[2];
      
      # Fit the simulated observation
      input_data <- list("y" = simu_y)
      
      capture.output(library(rstan))
      capture.output(fit <- sampling(fit_model, data=input_data, seed=4938483))
      
      # Compute diagnostics
      util <- new.env()
      source('stan_utility.R', local=util)
      
      warning_code <- util$check_all_diagnostics(fit, quiet=TRUE)
      
      # Compute rank of prior draw with respect to thinned posterior draws
      sbc_rank <- sum(simu_theta < extract(fit)$theta[seq(1, 4000 - 8, 8)])
      
      # Compute posterior sensitivities
      s <- summary(fit, probs = c(.9), pars='theta')$summary
      post_mean_theta <- s[,1]
      post_sd_theta <- s[,3]
      
      # From assumed Beta prior
      a <- 2; b <- 4
      prior_sd_theta <- sqrt(a * b/((a+b)^2 * (a+b+1)))
      
      z_score <- (post_mean_theta - simu_theta) / post_sd_theta
      shrinkage <- 1 - (post_sd_theta / prior_sd_theta)**2
      
      c(warning_code, sbc_rank, z_score, shrinkage, post_mean_theta, post_sd_theta, s[,4])
    }
}, finally={ stopImplicitCluster() })

type1error <- mean((ensemble_output[5,] - .5)/ensemble_output[6,] < -1.65)
print(paste0("Type 1 Error = ",type1error))

hist(ensemble_output[7,],breaks = 30,main="Histogram of 90th percentile of posteriors calculated\nwhen True Prior and Posterior equal .5 with probability 1",xlab="Calculated 90th percentile of posterior")

```


Model Adequacy: Checking the posterior itself.
========================================================
Not much to see with this toy example.

```{r  rstan7, cache = TRUE, results = "hide", message = FALSE, warning = FALSE, echo=FALSE}
modelString = "
data {
int y;
}

parameters {
real<lower=0,upper=1> theta; // Probability of Fainting
}

model {
theta ~ beta(2,4); // Prior model
y ~ binomial(9,theta);         // Observational model
}

// Simulate a full observation from the current value of the parameters
generated quantities {
int y_ppc;

y_ppc = binomial_rng(9,theta);
}
"
fit_model = stan_model(model_code=modelString)

simu_data <- list("y" = 3)

fit <- sampling( object=fit_model , 
  data = simu_data , 
  chains = 4 ,
  warmup = 100 )

params = extract(fit)

tt <- table(params$y_ppc)
data <- data.frame(tt)
dimnames(data)[[2]] <- c("x","y")

ggplot(data, aes(x=x, y=y)) +
  geom_point() + 
  geom_segment( aes(x=x, xend=x, y=0, yend=y)) +
  ggtitle("Posterior Predictive Distribution") +
  xlab("Predicted Faints in 9 business advisers.")

```


Posterior plot.
========================================================

```{r  rstan8, cache = TRUE, dependson = "stan4a", echo=FALSE}

params <- data.frame(params)
ii <- seq(.01,.99,.01)

prl <- density(params$theta,from = 0, to = 1)
plot(ii,dbeta(ii,5,10),type="n",main = "Density estimate from posterior sample against closed form posterior (in red)",xlab = "Faint proportion",ylab = "Density")
lines(prl$x,prl$y)
lines(ii,dbeta(ii,5,10),col=2)


```

Its a developing field - checking accuracy of posterior quantiles.
========================================================
Check Stan website if a concern.

```{r  rstan9, cache = TRUE, dependson = "stan8", echo=FALSE}

plot(seq(.01,.99,.03),quantile(params$theta,probs = seq(.01,.99,.03),5,10),type="n",main = "Percentile estimate from posterior sample against closed form percentiles (in red)",xlab = "Cumulative Faint proportion",ylab = "Percentile")
lines(seq(.01,.99,.03),qbeta(seq(.01,.99,.03),5,10))
lines(seq(.01,.99,.03),quantile(params$theta,probs = seq(.01,.99,.03),5,10),col=2)

```

Summary: Bayes enables powerful analyses.
========================================================
With great power comes great responsibility.

- Safe Bayes requires adequate assessment of:
- The joint model specified. 
- The posterior sampling carried out (usually MCMC). 
- The adequacy of the study design/data. 
- This is a developing field - one needs to stay current. 

Summary (cont.).
========================================================

- The joint model defines a set of of fake universes and what repeatedly happens in them. 
- That set should (!must) include mostly *close enough to our universe* fake universes. 
- If not the posterior might (!will) be misleading for our universe. 
- Two stage sampling provides a way to assess if fake universes look like ours.

Summary (cont.).
========================================================

- The posterior needs to be adequately sampled from. 
- Two stage sampling offers an additional direct assessment tool of whether sampling from the posterior was adequate.
- Extraction of information cannot exceed what is actually in the data (if it does, its error not signal). 
- Two stage sampling offers a way to assess the distribution of information in the study design/data. 

Summary (cont.).
========================================================

- How often the Bayesian analysis will be misleading is important.
- Smart people don't like being repeatedly wrong (Don Rubin). 
- Study designs (especially with large sample sizes) can mitigate a poor set of fake universes.
- Inadequate posterior sampling and poor data likely fatal.
- But don't assume or take anyone's word for it - check!



